import streamlit as st
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from langchain.agents import AgentExecutor, create_react_agent
from langchain.tools import tool
from langchain.prompts import PromptTemplate
# 1. å¯¼å…¥æ–°çš„HuggingFacePipeline
from langchain_huggingface import HuggingFacePipeline
import os
import sys

# --- å°†agentç›®å½•æ·»åŠ åˆ°Pythonçš„æœç´¢è·¯å¾„ä¸­ ---
agent_dir = os.path.join(os.path.dirname(__file__), 'agent')
sys.path.append(agent_dir)
from weather_tools import get_average_temperature_for_china

# --- é…ç½®å’Œå¸¸é‡ ---
MODEL_PATH = r"E:\Project_Collection\2025\WEATHER\model\1.5B"
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'


# --- å®šä¹‰Agentå¯ä»¥ä½¿ç”¨çš„å·¥å…· ---
@tool
def china_average_temperature_tool(time_interval: str, step: int) -> str:
    """
    å½“ç”¨æˆ·æƒ³è¦æŸ¥è¯¢æœªæ¥æŸä¸ªç‰¹å®šæ—¶é—´ç‚¹'ä¸­å›½'çš„'å¹³å‡æ¸©åº¦'æ—¶ï¼Œä½¿ç”¨æ­¤å·¥å…·ã€‚
    è¿™ä¸ªå·¥å…·éœ€è¦ä¸¤ä¸ªå‚æ•°ï¼š
    - 'time_interval': å­—ç¬¦ä¸²ç±»å‹ï¼Œè¡¨ç¤ºé¢„æµ‹çš„æ—¶é—´é—´éš”ï¼Œå¿…é¡»æ˜¯ '1h', '3h', '6h' æˆ– '24h' ä¸­çš„ä¸€ä¸ªã€‚
    - 'step': æ•´æ•°ç±»å‹ï¼Œè¡¨ç¤ºé¢„æµ‹çš„æ­¥æ•°ï¼Œä»1å¼€å§‹ã€‚
    ä¾‹å¦‚ï¼Œè¦æŸ¥è¯¢6å°æ—¶æ¨¡å‹çš„ç¬¬2æ­¥ï¼ˆå³æœªæ¥12å°æ—¶ï¼‰çš„æ•°æ®ï¼Œå‚æ•°åº”ä¸º time_interval='6h', step=2ã€‚
    """
    return get_average_temperature_for_china(time_interval, step)


# --- ç¼“å­˜èµ„æºåŠ è½½å‡½æ•° (éå¸¸é‡è¦) ---
@st.cache_resource
def load_llm_and_tokenizer():
    """åŠ è½½æœ¬åœ°LLMå’ŒTokenizerã€‚"""
    st.info(f"å¼€å§‹åŠ è½½æ¨¡å‹: {MODEL_PATH} åˆ° {DEVICE}...")
    try:
        # 2. ä¿æŒå¼ºåˆ¶ä½¿ç”¨CUDA
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_PATH,
            torch_dtype=torch.bfloat16,
            device_map=DEVICE  # ç›´æ¥ä½¿ç”¨æˆ‘ä»¬å®šä¹‰çš„DEVICEå˜é‡
        )
        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)

        # 3. åˆ›å»ºLLMå¯¹è±¡çš„æ–¹å¼å‘ç”Ÿæ”¹å˜
        # æˆ‘ä»¬ä¸å†åˆ›å»ºä¸€ä¸ªtransformersçš„pipelineï¼Œè€Œæ˜¯ç›´æ¥å°†æ¨¡å‹å’Œtokenizeräº¤ç»™LangChain
        # è¿™ç»™äº†æˆ‘ä»¬æ›´ç²¾ç»†çš„æ§åˆ¶
        llm = HuggingFacePipeline.from_model_id(
            model_id=MODEL_PATH,
            model_kwargs={"torch_dtype": torch.bfloat16},
            # device_map=DEVICE ä¼šè‡ªåŠ¨è¢«å¤„ç†ï¼Œç¡®ä¿æ¨¡å‹åœ¨GPUä¸Š
            # HuggingFacePipeline ä¼šæ›´æ™ºèƒ½åœ°å¤„ç†è®¾å¤‡æ”¾ç½®
            task="text-generation",
            pipeline_kwargs={
                "max_new_tokens": 1024,
                "do_sample": False,
                "temperature": 0.1,
                "top_p": 0.95,
            },
        )
        st.success("æ¨¡å‹å’ŒTokenizeråŠ è½½æˆåŠŸï¼")
        return llm
    except Exception as e:
        st.error(f"åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
        st.stop()


# --- åˆ›å»ºAgent (è¿™éƒ¨åˆ†ä»£ç ä¿æŒä¸å˜) ---
def create_pangu_agent(llm):
    tools = [china_average_temperature_tool]
# è¿™æ˜¯æˆ‘ä»¬çš„æ–°templateï¼Œå¯¹Action Inputæå‡ºäº†æ›´ä¸¥æ ¼çš„è¦æ±‚
    template = """
Answer the following questions as best you can. You have access to the following tools:

{tools}

Use the following format:

Question: the input question you must answer
Thought: you should always think about what to do
Action: the action to take, should be one of [{tool_names}]
Action Input: The input for the action. IMPORTANT: This MUST be a single, valid JSON object. All keys and all string values in the JSON MUST be enclosed in double quotes. Do not add any text before or after the JSON object.
Example: {{"time_interval": "6h", "step": 1}}
Observation: the result of the action
... (this Thought/Action/Action Input/Observation can repeat N times)
Thought: I now know the final answer
Final Answer: the final answer to the original question

Begin!

Question: {input}
Thought:{agent_scratchpad}
"""

    prompt = PromptTemplate.from_template(template)
    agent = create_react_agent(llm, tools, prompt)
    agent_executor = AgentExecutor(
        agent=agent,
        tools=tools,
        verbose=True,
        handle_parsing_errors=True
    )
    return agent_executor


# --- Streamlit åº”ç”¨ç•Œé¢ (è¿™éƒ¨åˆ†ä»£ç ä¿æŒä¸å˜) ---
st.set_page_config(page_title="ç›˜å¤æ°”è±¡æ™ºèƒ½åŠ©æ‰‹", page_icon="ğŸ§ ")
st.title("ç›˜å¤æ°”è±¡æ™ºèƒ½åŠ©æ‰‹ ğŸ’¬")
st.caption("ä¸€ä¸ªèƒ½ä¸ç›˜å¤æ¨¡å‹æ•°æ®å¯¹è¯çš„AI Agent")

llm = load_llm_and_tokenizer()
agent_executor = create_pangu_agent(llm)

if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant",
                                  "content": "ä½ å¥½ï¼æˆ‘æ˜¯ç›˜å¤æ°”è±¡æ™ºèƒ½åŠ©æ‰‹ã€‚ä½ å¯ä»¥é—®æˆ‘å…³äºæœªæ¥ä¸­å›½å¹³å‡æ¸©åº¦çš„é—®é¢˜ï¼Œä¾‹å¦‚ï¼š'æœªæ¥6å°æ—¶åä¸­å›½çš„å¹³å‡æ¸©åº¦æ˜¯å¤šå°‘ï¼Ÿ'"}]

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("è¯·è¾“å…¥ä½ çš„é—®é¢˜..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner("ğŸ§  æ™ºèƒ½åŠ©æ‰‹æ­£åœ¨æ€è€ƒå¹¶è®¡ç®—..."):
            response = agent_executor.invoke({"input": prompt})
            final_answer = response['output']
            st.markdown(final_answer)
            st.session_state.messages.append({"role": "assistant", "content": final_answer})

